{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####################################################\n",
    "#\n",
    "# 基于矩阵分解（Matrix Factorization，MF）的推荐算法\n",
    "# 对数据集MovieLens-25m 进行探索，以及矩阵分解模型的探索\n",
    "# \n",
    "# #####################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1、导入模块和包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这些模块和包都是在逐步的探索中所需要的，然后全部汇总到这里，\n",
    "#    并不是一开始就知道了 ^_^ ^_^ ^_^\n",
    "# 1、导入模块和包\n",
    "import pandas as pd    # 加载并处理csv文件\n",
    "import datetime        # 利用datetime处理时间戳\n",
    "import _pickle as cPickle    # 数据以二进制进行高效的储存到文件\n",
    "from collections import defaultdict     # 利用Python设置稀疏矩阵的NULL位置的默认值\n",
    "import scipy.sparse as ss     # 利用scipy构建稀疏矩阵\n",
    "import scipy.io as sio    # 利用scipy储存评分矩阵\n",
    "import numpy as np    # 利用numpy创建指定长度或形状的矩阵以及矩阵运算\n",
    "from numpy.random import random    # numpy.random中的randn函数生成一些正态分布的随机数据\n",
    "import time    # 利用Python内置模块，计算训练时迭代的时间\n",
    "import json    # 将模型参数保存为json文件，加载模型参数json文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2、加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xba in position 10: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xba in position 10: invalid start byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-cb3dc5dee2df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# pandas读取文件ratings.csv(两千五百多万行数据)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mcsv_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"BX-Book-Ratings.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\";\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2035\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2036\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2037\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2038\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2039\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xba in position 10: invalid start byte"
     ]
    }
   ],
   "source": [
    "# 2、加载数据集\n",
    "\n",
    "# 设置数据储存位置\n",
    "data_path = \"./dataset/BX-CSV-Dump/\"\n",
    "\n",
    "# pandas读取文件ratings.csv(两千五百多万行数据)\n",
    "csv_file = pd.read_csv(data_path + \"BX-Book-Ratings.csv\", sep = \";\", encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of user: 1210271\n",
      "the number of movie: 249274\n"
     ]
    }
   ],
   "source": [
    "# 计算用户数量和电影数量\n",
    "unique_user = csv_file['User-ID'].unique()\n",
    "unique_item = csv_file['ISBN'].unique()\n",
    "\n",
    "user_num = unique_user.shape[0]\n",
    "item_num = unique_item.shape[0]\n",
    "\n",
    "print(\"the number of user:\",user_num)\n",
    "print(\"the number of movie:\",item_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000095\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# 基本维度参数定义\n",
    "print(csv_file.shape[0])\n",
    "print(csv_file.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看数据维度（行，列）\n",
    "csv_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将时间戳这个字段属性删除\n",
    "csv_file = csv_file.drop(['timestamp'], axis=1)\n",
    "\n",
    "# 查看数据维度（行，列）\n",
    "csv_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 4)\n",
      "(8000, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>split_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8028</th>\n",
       "      <td>61</td>\n",
       "      <td>2606</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4501</th>\n",
       "      <td>29</td>\n",
       "      <td>562</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2053</th>\n",
       "      <td>12</td>\n",
       "      <td>2149</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3551</th>\n",
       "      <td>19</td>\n",
       "      <td>93240</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4122</th>\n",
       "      <td>23</td>\n",
       "      <td>2112</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      userId  movieId  rating  split_index\n",
       "8028      61     2606     2.0            0\n",
       "4501      29      562     3.5            0\n",
       "2053      12     2149     1.0            0\n",
       "3551      19    93240     3.5            0\n",
       "4122      23     2112     5.0            0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据集划分训练集和测试集\n",
    "\n",
    "# shuffle = True ：Ture 为打乱数据集进行划分，False为不打乱数据集划分\n",
    "def random_split (df, ratios, shuffle = True):\n",
    "    \n",
    "    # Function to split pandas DataFrame into train, validation and test\n",
    "    #\n",
    "    # Params:     \n",
    "    #    df (pd.DataFrame): Pandas data frame to be split.\n",
    "    #    ratios (list of floats): list of ratios for split. The ratios have to sum to 1.\n",
    "    #\n",
    "    # Returns: \n",
    "    #    list: List of pd.DataFrame split by the given specifications.\n",
    "    # ###################################################################################   \n",
    "    \n",
    "    seed = 42                  # Set random seed\n",
    "    if shuffle == True:\n",
    "        df = df.sample(frac=1)     # Shuffle the data\n",
    "    samples = df.shape[0]      # Number of samples\n",
    "    \n",
    "    # Converts [0.7, 0.2, 0.1] to [0.7, 0.9]\n",
    "    split_ratio = np.cumsum(ratios).tolist()[:-1] # Get split index\n",
    "    \n",
    "    # Get the rounded integer split index\n",
    "    split_index = [round(x * samples) for x in split_ratio]\n",
    "    \n",
    "    # split the data\n",
    "    splits = np.split(df, split_index)\n",
    "    \n",
    "    # Add split index (this makes splitting by group more efficient).\n",
    "    for i in range(len(ratios)):\n",
    "        splits[i][\"split_index\"] = i\n",
    "\n",
    "    return splits\n",
    "\n",
    "# 划分数据集\n",
    "train, test = random_split(csv_file, [0.8, 0.2])\n",
    "\n",
    "# 保存数据集为训练集和测试集\n",
    "# 利用pandas的 DataFrame的to_csv方法，将数据写到一个以逗号分隔的文件中\n",
    "train.to_csv(data_path + \"train.csv\")\n",
    "test.to_csv(data_path + \"test.csv\")\n",
    "\n",
    "print(test.shape)\n",
    "print(train.shape)\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>75</td>\n",
       "      <td>736</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>75</td>\n",
       "      <td>778</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>75</td>\n",
       "      <td>783</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>75</td>\n",
       "      <td>805</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>75</td>\n",
       "      <td>832</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      userId  movieId  rating\n",
       "9995      75      736     4.0\n",
       "9996      75      778     3.0\n",
       "9997      75      783     3.0\n",
       "9998      75      805     3.5\n",
       "9999      75      832     3.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3、数据处理——构建评分矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of user: 75\n",
      "the number of movie: 3287\n"
     ]
    }
   ],
   "source": [
    "# 3、数据处理——构建评分矩阵\n",
    "\n",
    "# 计算用户数量和电影数量\n",
    "unique_user = csv_file['userId'].unique()\n",
    "unique_item = csv_file['movieId'].unique()\n",
    "\n",
    "user_num = unique_user.shape[0]\n",
    "item_num = unique_item.shape[0]\n",
    "\n",
    "print(\"the number of user:\",user_num)\n",
    "print(\"the number of movie:\",item_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立用户和物品的索引表\n",
    "# 本数据集中user_id和item_id都已经是索引了,可以减1，将从1开始编码变成从0开始的编码\n",
    "# 下面的代码更通用，可对任意编码的用户和物品重新索引\n",
    "user_index = dict()    # 使用字典索引 保存用户索引\n",
    "item_index = dict()    # 保存电影索引\n",
    "\n",
    "for index, user in enumerate(unique_user):\n",
    "    user_index[user] = index\n",
    "    \n",
    "# 重新编码活动索引字典  \n",
    "# 序列函数，Python有一些有用的序列函数。\n",
    "# enumerate函数，迭代一个序列时， 你可能想跟踪当前项的序号。 \n",
    "# 因为这么做很常见， Python内建了一个 enumerate 函数， 可以返回 (i, value) 元组序列：\n",
    "# 当你索引数据时， 使用 enumerate 的一个好方法是计算序列（唯一的） dict 映射到位置的值：\n",
    "\n",
    "# 重新编码活动索引字典 \n",
    "for index, item in enumerate(unique_item):\n",
    "    item_index[item] = index\n",
    "    \n",
    "# 保存重新索引的用户索引表\n",
    "cPickle.dump(user_index, open(data_path + \"user_index.pkl\", 'wb'))\n",
    "# 保存重新索引的电影索引表\n",
    "cPickle.dump(item_index, open(data_path + \"item_index.pkl\", 'wb'))\n",
    "\n",
    "# 查看数据保存的情况\n",
    "#print(user_index.items())\n",
    "#print(\"==============================\")\n",
    "#print(item_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set, {})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_users = defaultdict(set)\n",
    "item_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 默认值，下面的逻辑很常见：\n",
    "# 关于设定值， 常见的情况是在字典的值是属于其它集合， 如列表。\n",
    "# collections 模块有一个很有用的类， defaultdict ，可以传递类型或函数以生成每个位置的默认值\n",
    "# setdefault 方法就正是干这个的,这两个有着异曲同工之妙\n",
    "\n",
    "# 倒排表\n",
    "# 统计每个用户打过分的电影   / 每个电影被哪些用户打过分\n",
    "user_items = defaultdict(set)\n",
    "item_users = defaultdict(set)\n",
    "\n",
    "# 用户-物品关系矩阵R, 稀疏矩阵，记录用户对每个电影的打分\n",
    "# user_num —— 代表m行， item_num —— 代表n列，构建 m*n 矩阵\n",
    "user_item_score = ss.dok_matrix((user_num, item_num))\n",
    "\n",
    "# 扫描数据，进行记录\n",
    "for row in csv_file.index:    # csv_file.index > RangeIndex(start=0, stop=10000, step=1)\n",
    "    # 获取数据的字段属性\n",
    "    row_info = csv_file.iloc[row]\n",
    "    \n",
    "    user_id = row_info[\"userId\"]\n",
    "    item_id = row_info[\"movieId\"]\n",
    "    rating = row_info[\"rating\"]\n",
    "    \n",
    "    # 当前用户索引，利用重新索引表\n",
    "    current_user_index = user_index[user_id]\n",
    "    # 当前电影索引，利用重新索引表\n",
    "    current_item_index = item_index[item_id]\n",
    "    \n",
    "    # 倒排表\n",
    "    user_items[current_user_index].add(current_item_index)    # 该用户对这个电影进行了打分\n",
    "    item_users[current_item_index].add(current_user_index)    # 该电影被该用户打分\n",
    "    \n",
    "    # 当前用户对当前电影的评分\n",
    "    user_item_score[current_user_index, current_item_index] = rating    \n",
    "\n",
    "\n",
    "# 保存倒排表\n",
    "# 每个用户打分的电影\n",
    "cPickle.dump(user_items, open(data_path + \"user_items.pkl\", 'wb'))\n",
    "# 对每个电影打过分的用户\n",
    "cPickle.dump(item_users, open(data_path + \"item_users.pkl\", 'wb'))\n",
    "\n",
    "# 保存评分矩阵，以便于后面使用\n",
    "sio.mmwrite(data_path + \"user_item_score\", user_item_score)\n",
    "\n",
    "# 查看数据保存的情况\n",
    "#print(user_items)\n",
    "#print(\"==============================\")\n",
    "#print(user_items)\n",
    "#print(\"==============================\")\n",
    "#print(user_item_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4、模型探索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1、BiasSVD模型 r(ui) = average + bu + bi + Pu'T Qi\n",
    "\n",
    "### 4.1.1、初始化BiasSVD模型参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4、模型探索\n",
    "\n",
    "# 4.1、BiasSVD模型 r(ui) = average + bu + bi + Pu'T Qi\n",
    "\n",
    "# 4.1.1、初始化BiasSVD模型参数\n",
    "\n",
    "# 隐含变量的维数 m*n = m*k * k*n\n",
    "# 设置参数变量，可以随时调整模型参数值\n",
    "k = 40\n",
    "\n",
    "# Item和User的偏置项\n",
    "# zeros和ones分别可以创建指定长度或形状的全0或全1数组。\n",
    "bi = np.zeros((item_num, 1))    # n 行 1 列\n",
    "bu = np.zeros((user_num, 1))    # m 行 1 列\n",
    "\n",
    "# Item和User的隐含向量\n",
    "qi = np.zeros((item_num, k))    # n 行 k 列    \n",
    "pu = np.zeros((user_num, k))    # m 行 k 列\n",
    "\n",
    "# 使用numpy.random中的randn函数生成一些正态分布的随机数据\n",
    "# pu和qi，两个矩阵的初始化，可以使全为零\n",
    "# 也可以符合正态分布（高斯分布），同时也隐向量的温度k的平方有关，故此可以初始化\n",
    "for user_id in range(user_num):\n",
    "    pu[user_id] = np.reshape(random((k, 1)) / 10 * np.sqrt(k), k)\n",
    "       \n",
    "for item_id in range(item_num):\n",
    "    qi[item_id] = np.reshape(random((k, 1)) / 10 * np.sqrt(k), k)\n",
    "\n",
    "# 所有用户的平均打分\n",
    "average = csv_file['rating'].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2、根据初始化BiasSVD模型参数进行预测评分\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1.2、根据初始化BiasSVD模型参数进行预测评分\n",
    "# 根据当前模型参数，预测用户对物品打分\n",
    "def BiasSVD_prediction(user_id, item_id):  \n",
    "    score = average + bi[item_id] + bu[user_id] + np.sum(qi[item_id] * pu[user_id]) \n",
    "    return score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id = user_index[ csv_file.iloc [row] ['userId'] ]\n",
    "user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId      75.0\n",
       "movieId    832.0\n",
       "rating       3.0\n",
       "Name: 9999, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_id = csv_file.iloc [row] \n",
    "item_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3、BiasSVD模型训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始进行5个step的训练\n",
      "The 0-th  step is running\n",
      "完成第1个step的训练, rmse=[1.1404187], 耗时15.5349秒\n",
      "The 1-th  step is running\n",
      "完成第2个step的训练, rmse=[0.88212268], 耗时15.1859秒\n",
      "The 2-th  step is running\n",
      "完成第3个step的训练, rmse=[0.82623763], 耗时14.4428秒\n",
      "The 3-th  step is running\n",
      "完成第4个step的训练, rmse=[0.78743791], 耗时14.5438秒\n",
      "The 4-th  step is running\n",
      "完成第5个step的训练, rmse=[0.75417417], 耗时14.4078秒\n",
      "结束了5个step的训练，总耗时74.1162秒\n"
     ]
    }
   ],
   "source": [
    "# 4.1.3、BiasSVD模型训练\n",
    "\n",
    "# BiasSVD模型超参数：gamma —— 学习率、Lambda —— 正则参数、steps —— 迭代次数\n",
    "# 初始化BiasSVD模型超参数\n",
    "steps = 5\n",
    "gamma = 0.04\n",
    "Lambda = 0.15\n",
    "\n",
    "# 总的评分记录数目\n",
    "total_records = csv_file.shape[0]\n",
    "\n",
    "# 第一次迭代时间开始\n",
    "time_start = time.time()\n",
    "\n",
    "print(\"开始进行{}个step的训练\".format(steps))\n",
    "\n",
    "each_time_start = time_start\n",
    "for step in range(steps):\n",
    "    print('The {}-th  step is running'.format(step))\n",
    "    rmse_sum = 0.0    # 真实评分与预测评分的差值的平方            \n",
    "    \n",
    "    # 利用numpy.random.permutation函数可以轻松实现对Series或DataFrame的列的排列工作permuting， 随机重排序）\n",
    "    # 通过需要排列的轴的长度调用permutation， 可产生一个表示新顺序的整数数组\n",
    "    # 将训练样本打散顺序 \n",
    "    total_num = np.random.permutation(total_records)  \n",
    "    for index in range(total_records):\n",
    "        # 每次一个训练样本,即一条记录\n",
    "        row = total_num[index]\n",
    "        \n",
    "        user_id = user_index[ csv_file.iloc [row] ['userId'] ]\n",
    "        item_id = item_index[ csv_file.iloc [row] ['movieId'] ]\n",
    "        rating = csv_file.iloc[row]['rating']\n",
    "        \n",
    "        # 目标函数构建\n",
    "        # 预测残差\n",
    "        eui = rating - BiasSVD_prediction(user_id, item_id)\n",
    "        # residual sum of squares 残差平方和 \n",
    "        rmse_sum += eui**2\n",
    "\n",
    "        # 随机梯度下降，更新\n",
    "        bu[user_id] += gamma * (eui - Lambda * bu[user_id])  \n",
    "        bi[item_id] += gamma * (eui - Lambda * bi[item_id])\n",
    "                \n",
    "        temp = qi[item_id]  \n",
    "        qi[item_id] += gamma * (eui * pu[user_id] - Lambda * qi[item_id])\n",
    "        pu[user_id] += gamma * (eui * temp - Lambda * pu[user_id])\n",
    "            \n",
    "    # 学习率递减\n",
    "    gamma = gamma * 0.93\n",
    "    # Root Mean Square Error RMSE\n",
    "    each_rmse = np.sqrt(rmse_sum / total_records)\n",
    "    \n",
    "    # 每次迭代时间结束\n",
    "    each_time_tick = time.time()\n",
    "    # 每次迭代消耗的时间\n",
    "    each_cost_time = each_time_tick - each_time_start\n",
    "    # 更新计算每次迭代的时间\n",
    "    each_time_start = each_time_tick\n",
    "\n",
    "    print(\"完成第{}个step的训练, rmse={}, 耗时{:.4f}秒\".format(\n",
    "        step + 1, each_rmse, each_cost_time))\n",
    "\n",
    "# 计算训练数据集消耗的总时间\n",
    "time_end = time.time()\n",
    "total_cost_time = time_end - time_start\n",
    "print(\"结束了{}个step的训练，总耗时{:.4f}秒\".format(steps, total_cost_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.67576256]\n",
      " [ 0.23023437]\n",
      " [ 0.16621839]\n",
      " ...\n",
      " [ 0.14021747]\n",
      " [-0.10993499]\n",
      " [-0.13186611]]\n",
      "[[0.10514672 0.10433964 0.21246684 ... 0.01330342 0.13103987 0.15793642]\n",
      " [0.25326122 0.54388725 0.17494823 ... 0.24567763 0.22499317 0.43867627]\n",
      " [0.18921225 0.26408512 0.16073564 ... 0.42450137 0.1446154  0.19650303]\n",
      " ...\n",
      " [0.10548351 0.3494358  0.48915359 ... 0.51669651 0.07883793 0.55501072]\n",
      " [0.47214483 0.59626442 0.54172038 ... 0.42103358 0.03210538 0.46993091]\n",
      " [0.27956149 0.06616051 0.54179079 ... 0.15729067 0.30033023 0.5293191 ]]\n"
     ]
    }
   ],
   "source": [
    "import json    # 将模型参数保存为json文件，加载模型参数json文件\n",
    "print(bi)\n",
    "print(qi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4、BiasSVD模型参数保存\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1.4、BiasSVD模型参数保存\n",
    "\n",
    "# 将 BiasSVD 模型参数保存为json， 键（模型参数） 和 值（模型参数值）\n",
    "def BiasSVD_parameter_toJson(filepath):\n",
    "    BiasSVD_parameter_dict = dict()\n",
    "    BiasSVD_parameter_dict['average'] = average\n",
    "    BiasSVD_parameter_dict['k'] = k\n",
    "    \n",
    "    # numpy.ndarray.map() 将列表转换为矩阵\n",
    "    # numpy.ndarray.tolist() 将矩阵转换为列表\n",
    "    BiasSVD_parameter_dict['bi'] = bi.tolist()\n",
    "    BiasSVD_parameter_dict['bu'] = bu.tolist()\n",
    "    \n",
    "    BiasSVD_parameter_dict['qi'] = qi.tolist()\n",
    "    BiasSVD_parameter_dict['pu'] = pu.tolist()\n",
    "\n",
    "    # json.loads即可将JSON字符串转换成Python形式\n",
    "    # json.dumps则将Python对象转换成JSON格式\n",
    "    BiasSVD_parameter_toJsonTxt = json.dumps(BiasSVD_parameter_dict)\n",
    "    with open(filepath, 'w') as file:\n",
    "        file.write(BiasSVD_parameter_toJsonTxt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1.5、BiasSVD模型参数加载\n",
    "\n",
    "def BiasSVD_parameter_load_fromJson(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        BiasSVD_parameter_dict = json.load(file)\n",
    "\n",
    "        average = BiasSVD_parameter_dict['average']\n",
    "        K = BiasSVD_parameter_dict['k']\n",
    "\n",
    "        # numpy.asarray 将数据转化为 ndarray 格式\n",
    "        bi = np.asarray(BiasSVD_parameter_dict['bi'])\n",
    "        bu = np.asarray(BiasSVD_parameter_dict['bu'])\n",
    "    \n",
    "        qi = np.asarray(BiasSVD_parameter_dict['qi'])\n",
    "        pu = np.asarray(BiasSVD_parameter_dict['pu'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1.4、BiasSVD模型参数保存\n",
    "BiasSVD_parameter_toJson(data_path + 'BiasSVD_modelParameter.json')\n",
    "\n",
    "# 4.1.5、BiasSVD模型参数加载\n",
    "BiasSVD_parameter_load_fromJson(data_path + 'BiasSVD_modelParameter.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1.6、BiasSVD模型参数传递给预测函数\n",
    "\n",
    "# 4.1.2、根据初始化BiasSVD模型参数进行预测评分\n",
    "# 根据当前模型参数，预测用户对物品打分\n",
    "#def BiasSVD_prediction(user_id, item_id):  \n",
    " #   score = average + bi[item_id] + bu[user_id] + np.sum(qi[item_id] * pu[user_id]) \n",
    "  #  return score \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1.7、BiasSVD模型读取测试集数据\n",
    "\n",
    "#test_set_data = pd.read_csv(data_path + \"test.csv\")\n",
    "#test_set_data = test_set_data.drop(['timestamp'], axis=1)\n",
    "#test_set_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "## Prediction Metrics (Similiar to Regression Problem)\n",
    "    - RMSE（均方根误差）\n",
    "    - MSE（均方误差）\n",
    "    - MAE（平均绝对误差）\n",
    "\n",
    "### Hit Metrics (Similiar to Classification Metrics)\n",
    "**Hit** \n",
    "- defined by relevancy, a hit usually means whether the recommended \"k\" items hit the \"relevant\" items by the user. \n",
    "\n",
    "For example, a user may have clicked, viewed, or purchased an item for many times, and a hit in the recommended items indicate that the recommender performs well. \n",
    "Metrics like \"precision\", \"recall\", etc. measure the performance of such hitting accuracy.\n",
    "\n",
    "    - Precision@k（精确率）\n",
    "    - Recall@k（召回率）\n",
    "  \n",
    "\n",
    "### Ranking Metrics\n",
    "\n",
    "**Ranking** \n",
    "- ranking metrics give more explanations about, for the hitted items, whether they are ranked in a way that is preferred by the users whom the items will be recommended to.\n",
    "\n",
    "Metrics like \"mean average precision\", \"ndcg\", etc., evaluate whether the relevant items are ranked higher than the less-relevant or irrelevant items. \n",
    "\n",
    "    - MeanReciprocalRank@k\n",
    "    - MeanAveragePrecision@k\n",
    "    - NDCG@k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.6、BiasSVD模型评测指标\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the MAE of the RS of the train set : [0.64297474]\n",
      "the RMSE of the RS of the train set : [0.90521144]\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "# 4.1.6、BiasSVD模型评测指标\n",
    "\n",
    "# 数据集的总的样本空间数 T\n",
    "# 真实的用户对物品的评分 rui\n",
    "# 预测的用户对物品的评分 r`ui\n",
    "# 真实评分与预测评分的差值 rss = rui - r`ui\n",
    "\n",
    "# 对 rss 进行绝对值得到 rss_absolute\n",
    "# 对 rss_absolute 进行求和得到 rss_absolute_sum\n",
    "# MAE = rss_absolute_sum / T\n",
    "\n",
    "# 对 rss 进行平方得到 rss_square\n",
    "# 对 rss_square 进行求和得到 rss_square_sum\n",
    "# RMSE = rss_square_sum / T\n",
    "# MSE = rss_square / T\n",
    "\n",
    "def calculate_MAE(dataset):\n",
    "    mae_sum = 0.0    # 真实评分与预测评分的差值的绝对值之和\n",
    "    T = dataset.shape[0]\n",
    "    total_num = np.random.permutation(T)\n",
    "    for index in range(T):\n",
    "        # 每次计算一个样本,即一条记录\n",
    "        row = total_num[index]\n",
    "        rating = dataset.iloc[row]['rating']\n",
    "        \n",
    "        # 预测残差\n",
    "        eui = rating - BiasSVD_prediction(user_id, item_id)\n",
    "        # residual sum of squares 残差平方和 \n",
    "        mae_sum += abs(eui)\n",
    "            \n",
    "    # Root Mean Square Error RMSE\n",
    "    MAE = mae_sum / total_records\n",
    "    return MAE\n",
    "    \n",
    "\n",
    "def calculate_RMSE(dataset):\n",
    "    rmse_sum = 0.0    # 真实评分与预测评分的差值的平方和\n",
    "    T = dataset.shape[0]\n",
    "    total_num = np.random.permutation(T)\n",
    "    for index in range(T):\n",
    "        # 每次计算一个样本,即一条记录\n",
    "        row = total_num[index]\n",
    "        rating = dataset.iloc[row]['rating']\n",
    "        \n",
    "        # 预测残差\n",
    "        eui = rating - BiasSVD_prediction(user_id, item_id)\n",
    "        # residual sum of squares 残差平方和 \n",
    "        rmse_sum += eui**2\n",
    "            \n",
    "    # Root Mean Square Error RMSE\n",
    "    RMSE = np.sqrt(rmse_sum / total_records)\n",
    "    return RMSE\n",
    "\n",
    "# 训练集评测指标：RMSE、MAE\n",
    "print(\"the MAE of the RS of the train set :\", calculate_MAE(train))\n",
    "print(\"the RMSE of the RS of the train set :\", calculate_RMSE(train))\n",
    "print(\"====================================================\")\n",
    "# 测试集评测指标：RMSE、MAE\n",
    "#print(\"the MAE of the RS of the test set :\", calculate_MAE(test))\n",
    "#print(\"the RMSE of the RS of the test set :\", calculate_RMSE(test))\n",
    "#print(\"====================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>split_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7502</th>\n",
       "      <td>59</td>\n",
       "      <td>1343</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>8</td>\n",
       "      <td>180</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3986</th>\n",
       "      <td>23</td>\n",
       "      <td>1094</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2807</th>\n",
       "      <td>13</td>\n",
       "      <td>69844</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6646</th>\n",
       "      <td>49</td>\n",
       "      <td>435</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8028</th>\n",
       "      <td>61</td>\n",
       "      <td>2606</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4501</th>\n",
       "      <td>29</td>\n",
       "      <td>562</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2053</th>\n",
       "      <td>12</td>\n",
       "      <td>2149</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3551</th>\n",
       "      <td>19</td>\n",
       "      <td>93240</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4122</th>\n",
       "      <td>23</td>\n",
       "      <td>2112</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      userId  movieId  rating  split_index\n",
       "7502      59     1343     3.5            0\n",
       "1332       8      180     4.0            0\n",
       "3986      23     1094     5.0            0\n",
       "2807      13    69844     3.5            0\n",
       "6646      49      435     3.0            0\n",
       "...      ...      ...     ...          ...\n",
       "8028      61     2606     2.0            0\n",
       "4501      29      562     3.5            0\n",
       "2053      12     2149     1.0            0\n",
       "3551      19    93240     3.5            0\n",
       "4122      23     2112     5.0            0\n",
       "\n",
       "[8000 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2、Funk SVD模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2.1、LFM模型\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.1、LMF模型\n",
    "\n",
    "# ##########################\n",
    "#\n",
    "# 核心算法实现\n",
    "#\n",
    "# @输入参数\n",
    "#     R —— M*N 评分矩阵\n",
    "#     k —— 隐向量的维度\n",
    "#     theta —— 迭代次数\n",
    "#     alpha —— 步长（学习率）\n",
    "#     lamda —— 正则化系数\n",
    "#\n",
    "# @输出参数\n",
    "#     分解之后的 P，Q\n",
    "#     P：初始化用户特征矩阵 M*K\n",
    "#     Q：初始化物品特征矩阵 N*K\n",
    "#\n",
    "# ##########################\n",
    "\n",
    "# 设定模型参数\n",
    "K = 5\n",
    "theta = 100\n",
    "alpha = 0.0002\n",
    "lamda = 0.004\n",
    "\n",
    "# 核心算法\n",
    "def LFM_grad_desc( R, K, theta, alpha, lamda ):\n",
    "    # 基本维度参数定义\n",
    "    M = R.shape[0]\n",
    "    N = R.shape[1]\n",
    "    \n",
    "    # P,Q初始值，随机生成\n",
    "    P = np.random.rand(M, K)\n",
    "    Q = np.random.rand(N, K)\n",
    "    Q = Q.T\n",
    "    \n",
    "    # 开始迭代\n",
    "    for step in range(theta):\n",
    "        print(\"开始第{}次迭代训练\".format(step))\n",
    "        # 对所有的用户u、物品i做遍历，对应的特征向量Pu、Qi梯度下降\n",
    "        for u in range(M):\n",
    "            for i in range(N):\n",
    "                # 对于每一个大于0的评分，求出预测评分误差\n",
    "                if R[u, i] > 0:\n",
    "                    eui = np.dot( P[u,:], Q[:,i] ) - R[u, i]\n",
    "                    \n",
    "                    # 代入公式，按照梯度下降算法更新当前的Pu、Qi\n",
    "                    for k in range(K):\n",
    "                        P[u][k] = P[u][k] - alpha * ( 2 * eui * Q[k][i] + 2 * lamda * P[u][k] )\n",
    "                        Q[k][i] = Q[k][i] - alpha * ( 2 * eui * P[u][k] + 2 * lamda * Q[k][i] )\n",
    "        \n",
    "        # u、i遍历完成，所有特征向量更新完成，可以得到P、Q，可以计算预测评分矩阵\n",
    "        predR = np.dot( P, Q )\n",
    "        \n",
    "        # 计算当前损失函数\n",
    "        cost = 0\n",
    "        for u in range(M):\n",
    "            for i in range(N):\n",
    "                if R[u, i] > 0:\n",
    "                    cost += ( np.dot( P[u,:], Q[:,i] ) - R[u, i] ) ** 2\n",
    "                    # 加上正则化项\n",
    "                    for k in range(K):\n",
    "                        cost += lamda * ( P[u][k] ** 2 + Q[k, i] ** 2 )\n",
    "        print(\"第{}次迭代结束\".format(step))\n",
    "        if cost < 0.0001:\n",
    "            break\n",
    "        \n",
    "    return P, Q.T, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.dok.dok_matrix"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(user_item_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.matrix"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "# todense() 转换为矩阵 numpy \n",
    "R = scipy.sparse.csc_matrix.todense(user_item_score)\n",
    "type(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始第0次迭代训练\n",
      "第0次迭代结束\n",
      "开始第1次迭代训练\n",
      "第1次迭代结束\n",
      "开始第2次迭代训练\n",
      "第2次迭代结束\n",
      "开始第3次迭代训练\n",
      "第3次迭代结束\n",
      "开始第4次迭代训练\n",
      "第4次迭代结束\n",
      "开始第5次迭代训练\n",
      "第5次迭代结束\n",
      "开始第6次迭代训练\n",
      "第6次迭代结束\n",
      "开始第7次迭代训练\n",
      "第7次迭代结束\n",
      "开始第8次迭代训练\n",
      "第8次迭代结束\n",
      "开始第9次迭代训练\n",
      "第9次迭代结束\n",
      "开始第10次迭代训练\n",
      "第10次迭代结束\n",
      "开始第11次迭代训练\n",
      "第11次迭代结束\n",
      "开始第12次迭代训练\n",
      "第12次迭代结束\n",
      "开始第13次迭代训练\n",
      "第13次迭代结束\n",
      "开始第14次迭代训练\n",
      "第14次迭代结束\n",
      "开始第15次迭代训练\n",
      "第15次迭代结束\n",
      "开始第16次迭代训练\n",
      "第16次迭代结束\n",
      "开始第17次迭代训练\n",
      "第17次迭代结束\n",
      "开始第18次迭代训练\n",
      "第18次迭代结束\n",
      "开始第19次迭代训练\n",
      "第19次迭代结束\n",
      "开始第20次迭代训练\n",
      "第20次迭代结束\n",
      "开始第21次迭代训练\n",
      "第21次迭代结束\n",
      "开始第22次迭代训练\n",
      "第22次迭代结束\n",
      "开始第23次迭代训练\n",
      "第23次迭代结束\n",
      "开始第24次迭代训练\n",
      "第24次迭代结束\n",
      "开始第25次迭代训练\n",
      "第25次迭代结束\n",
      "开始第26次迭代训练\n",
      "第26次迭代结束\n",
      "开始第27次迭代训练\n",
      "第27次迭代结束\n",
      "开始第28次迭代训练\n",
      "第28次迭代结束\n",
      "开始第29次迭代训练\n",
      "第29次迭代结束\n",
      "开始第30次迭代训练\n",
      "第30次迭代结束\n",
      "开始第31次迭代训练\n",
      "第31次迭代结束\n",
      "开始第32次迭代训练\n",
      "第32次迭代结束\n",
      "开始第33次迭代训练\n",
      "第33次迭代结束\n",
      "开始第34次迭代训练\n",
      "第34次迭代结束\n",
      "开始第35次迭代训练\n",
      "第35次迭代结束\n",
      "开始第36次迭代训练\n",
      "第36次迭代结束\n",
      "开始第37次迭代训练\n",
      "第37次迭代结束\n",
      "开始第38次迭代训练\n",
      "第38次迭代结束\n",
      "开始第39次迭代训练\n",
      "第39次迭代结束\n",
      "开始第40次迭代训练\n",
      "第40次迭代结束\n",
      "开始第41次迭代训练\n",
      "第41次迭代结束\n",
      "开始第42次迭代训练\n",
      "第42次迭代结束\n",
      "开始第43次迭代训练\n",
      "第43次迭代结束\n",
      "开始第44次迭代训练\n",
      "第44次迭代结束\n",
      "开始第45次迭代训练\n",
      "第45次迭代结束\n",
      "开始第46次迭代训练\n",
      "第46次迭代结束\n",
      "开始第47次迭代训练\n",
      "第47次迭代结束\n",
      "开始第48次迭代训练\n",
      "第48次迭代结束\n",
      "开始第49次迭代训练\n",
      "第49次迭代结束\n",
      "开始第50次迭代训练\n",
      "第50次迭代结束\n",
      "开始第51次迭代训练\n",
      "第51次迭代结束\n",
      "开始第52次迭代训练\n",
      "第52次迭代结束\n",
      "开始第53次迭代训练\n",
      "第53次迭代结束\n",
      "开始第54次迭代训练\n",
      "第54次迭代结束\n",
      "开始第55次迭代训练\n",
      "第55次迭代结束\n",
      "开始第56次迭代训练\n",
      "第56次迭代结束\n",
      "开始第57次迭代训练\n",
      "第57次迭代结束\n",
      "开始第58次迭代训练\n",
      "第58次迭代结束\n",
      "开始第59次迭代训练\n",
      "第59次迭代结束\n",
      "开始第60次迭代训练\n",
      "第60次迭代结束\n",
      "开始第61次迭代训练\n",
      "第61次迭代结束\n",
      "开始第62次迭代训练\n",
      "第62次迭代结束\n",
      "开始第63次迭代训练\n",
      "第63次迭代结束\n",
      "开始第64次迭代训练\n",
      "第64次迭代结束\n",
      "开始第65次迭代训练\n",
      "第65次迭代结束\n",
      "开始第66次迭代训练\n",
      "第66次迭代结束\n",
      "开始第67次迭代训练\n",
      "第67次迭代结束\n",
      "开始第68次迭代训练\n",
      "第68次迭代结束\n",
      "开始第69次迭代训练\n",
      "第69次迭代结束\n",
      "开始第70次迭代训练\n",
      "第70次迭代结束\n",
      "开始第71次迭代训练\n",
      "第71次迭代结束\n",
      "开始第72次迭代训练\n",
      "第72次迭代结束\n",
      "开始第73次迭代训练\n",
      "第73次迭代结束\n",
      "开始第74次迭代训练\n",
      "第74次迭代结束\n",
      "开始第75次迭代训练\n",
      "第75次迭代结束\n",
      "开始第76次迭代训练\n",
      "第76次迭代结束\n",
      "开始第77次迭代训练\n",
      "第77次迭代结束\n",
      "开始第78次迭代训练\n",
      "第78次迭代结束\n",
      "开始第79次迭代训练\n",
      "第79次迭代结束\n",
      "开始第80次迭代训练\n",
      "第80次迭代结束\n",
      "开始第81次迭代训练\n",
      "第81次迭代结束\n",
      "开始第82次迭代训练\n",
      "第82次迭代结束\n",
      "开始第83次迭代训练\n",
      "第83次迭代结束\n",
      "开始第84次迭代训练\n",
      "第84次迭代结束\n",
      "开始第85次迭代训练\n",
      "第85次迭代结束\n",
      "开始第86次迭代训练\n",
      "第86次迭代结束\n",
      "开始第87次迭代训练\n",
      "第87次迭代结束\n",
      "开始第88次迭代训练\n",
      "第88次迭代结束\n",
      "开始第89次迭代训练\n",
      "第89次迭代结束\n",
      "开始第90次迭代训练\n",
      "第90次迭代结束\n",
      "开始第91次迭代训练\n",
      "第91次迭代结束\n",
      "开始第92次迭代训练\n",
      "第92次迭代结束\n",
      "开始第93次迭代训练\n",
      "第93次迭代结束\n",
      "开始第94次迭代训练\n",
      "第94次迭代结束\n",
      "开始第95次迭代训练\n",
      "第95次迭代结束\n",
      "开始第96次迭代训练\n",
      "第96次迭代结束\n",
      "开始第97次迭代训练\n",
      "第97次迭代结束\n",
      "开始第98次迭代训练\n",
      "第98次迭代结束\n",
      "开始第99次迭代训练\n",
      "第99次迭代结束\n"
     ]
    }
   ],
   "source": [
    "P, Q, ess = LFM_grad_desc( R, K, theta, alpha, lamda )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_R = np.dot( P, Q.T )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[5. , 3.5, 5. , ..., 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "        [5. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "        ...,\n",
       "        [0. , 0. , 0. , ..., 5. , 3. , 3. ],\n",
       "        [4. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "        [3. , 0. , 0. , ..., 0. , 0. , 0. ]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.37959329, 4.0725477 , 4.18356673, ..., 4.69714686, 3.90617653,\n",
       "        2.61912681],\n",
       "       [4.59797716, 3.91564098, 4.48648135, ..., 4.42639821, 3.88860296,\n",
       "        2.5100853 ],\n",
       "       [4.68576784, 3.75297097, 4.38167271, ..., 4.61796773, 4.13431216,\n",
       "        2.68262305],\n",
       "       ...,\n",
       "       [4.31799322, 3.48791513, 4.20280659, ..., 4.11376324, 3.7330166 ,\n",
       "        2.41233162],\n",
       "       [3.75739151, 2.34562796, 3.34480073, ..., 2.91062119, 2.70598974,\n",
       "        1.6038933 ],\n",
       "       [3.94888511, 2.8223501 , 3.44333452, ..., 3.7319565 , 3.33734725,\n",
       "        2.12707708]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LFM模型评测指标\n",
    "# RMSE 和 MAE\n",
    "\n",
    "def calc_evaluation(R, pred_R):\n",
    "    mae_sum = 0.0\n",
    "    rmse_sum = 0.0\n",
    "    # 基本维度参数定义\n",
    "    M = R.shape[0]\n",
    "    N = R.shape[1]\n",
    "    T = 0\n",
    "    \n",
    "    for user in range(M):\n",
    "        for item in range(N):\n",
    "            if R[user, item] > 0:\n",
    "                T += 1\n",
    "                ess = R[user, item] - pred_R[user, item]\n",
    "                mae_sum += abs(ess)\n",
    "                rmse_sum += ess**2\n",
    "    \n",
    "    # 计算总的 MAE 指标\n",
    "    MAE = mae_sum / T\n",
    "    # Root Mean Square Error RMSE\n",
    "    RMSE = np.sqrt(rmse_sum / T)\n",
    "    return MAE, RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the MAE of LFM: 0.6362259409159522\n",
      "the RMSE OF LFM: 0.8156477605909996\n"
     ]
    }
   ],
   "source": [
    "MAE, RMSE = calc_evaluation(R, pred_R)\n",
    "print(\"the MAE of LFM:\", MAE)\n",
    "print(\"the RMSE OF LFM:\", RMSE) # 迭代次数为 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2089128969454637"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE # 迭代次数为 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.492576522778719"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
